# General Knowledge

- **Performance vs Scalability**
  - Service is scalable if resources added to system results in increased performance in a manner proportional to the resources added.
  - Performance problem if system is slow for a single user.
  - Scalability problem if system is fast for a single user but slow under heavy load.
    - **Vertical Scaling**: scale up, add resources to a single node system. Expensive and there is limitation
    - **Horizontal Scaling**: scale out, infinite hosts but have to deal with distributed systems.
- **Latency vs Throughput**
  - **Latency**: time to perform some action or to produce some result. Latency us measured in units of time - hours, minutes, seconds, nanoseconds, or clock periods.
  - **Throughput**: number of actions or results per unit of time. This is measured in units of whatever is being produced per unit of time.
  - Aim for maximal throughput with acceptable latency.
- **Availability vs Consistency**
  - **Consistency**: every read receives the most recent write or an error
    - **Weak Consistency**: after a write, reads may or may not see it.
      - Seen in systems such as memcached.
      - Works well with use cases like VoIP, video chat, and realtime multiplayer games.
    - **Eventual Consistency**: after a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously.
      - Seen in systems with high availability.
      - Works well with use cases such as DNS, email.
      - Amazon S3, SimpleDB
    - **Strong Consistency**: after a write, reads will see it. Data is replicated synchronously
      - Seen in systems that need transactions
      - Seen in file systems and Relational Data Management Systems (RDBMS)
  - **Availability**: every request receives a response, without a guarantee that it contains the most recent version of the information
    - **Fail-over**: standby equipment automatically takes over if main system fails. Disadvantages: more hardware and additional complexity, there is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive
      - **Active-passive**: heartbeats are sent between active and passive server on standby. If heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.
        - can also be referred to as master-slave failover.
      - **Active-active**: both servers are managing traffic, spreading the load between them.
        - can also be referred to as master-master failover.
    - **Replication**
      - **Master-slave**: master serves reads and write, replicating writes to one or more slaves, which serve only reads. Slaves can also replicate to additional slaves in a tree-like fashion. If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.
      - **Master-Master**: both masters serve reads and writes and collaborate with each other on write. If either master goes down, the system can continue to operate with both reads and writes.
      - [**Buddy Replication**](https://access.redhat.com/documentation/en-us/jboss_enterprise_application_platform/4.3/html/cache_tree_cache_guide/clustered_cache___using_replication-buddy_replication): suppress replicating your data to all instances in a cluster. Instead, each instance picks one or more 'buddies' in the cluster, and only replicates to these specific buddies. This greatly helps scalability as there is no longer a memory and network traffic impact every time another instance is added to a cluster.
        - Benefitial only if a certain data is frequently accessed it is served from one instance rather than a round-robin fashion. Ex: sticky sessions
  - **Partition Tolerance**: system continues to operate despite arbitrary partitioning due to network failures. Only network failure will cause system to respond incorrectly
  - **Brewer's CAP theorem**. In distributed system only support two of three at any given point in time. In centralized system we don't have network partions, so we can get both availability and consistency.
    - **Consistency and partition tolerance (CP)**: data is consistent between all nodes, and maintains partition tolerance (preventing data desync) by becoming unavailable when a node goes down. Waiting for a response from a partitioned node might result in a timeout error. CP is good choice if business needs atomic reads and writes. Use: HBase, MongoDB, Redis, MemcacheDB, BigTable-like systems
    - **Availability and Partition Tolerance (AP)**: nodes remain online even if they can't communicate with each other and will resync data once the partition is resolved, but you aren't guaranteed that all nodes will have the same data (either during or after the partition). Responses return the most recent version of the data available on a node, which might not be the latest. Writes might take some tome to propagate when the partition is resolved. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. Use: Voldemort, Riak, Cassandra, CouchDB, Dynamo-like systems
    - **Consistency and Availability (CA)**: not possible, it is a software tradeoff. You can choose what to do in the face of a network partition. Nodes remain online even if they can't communicate with each other and will resync data once the partition is resolved, but you aren't guaranteed that all nodes will have the same data (either during or after the partition). Use: Traditional relational databases like PostgreSQL, MySQL, etc.
- **Vertical Scaling**: scale up, add resources to a single node system. Expensive and there is hardware limitation. Ex: SQL
- **Horizontal Scaling**: scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single sever on more expensive hardware. Ex: load balancers, NoSQL
  - Disadvantages
    - scaling horizontally introduces complexity and involves cloning servers
      - servers should be stateless: they should not contain any user-related data like sessions or profile pictures
      - sessions can be stored in a centralized data store such as a database (SQL, NoSQL) or a persistent cache (Redis, Memcached)
    - downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out
- **Database**: electronic system that allows data to be easily accessed, manipulated, and updated.
  - **Relational Database Management System (RDBMS)**: relational database like SQL, Oracle is a collection of data items organized in tables. Highly-structued table organization with rigidly-defined data formats and record structure. Scales vertically, scaling reads difficult, scaling writes nearly impossible.
    - **ACID**: Set of properties of relational databse transactions intended to guarantee validity in the event of errors, power failures, etc.
      - **Atomicity**: each transaction is all of nothing
      - **Consistency**: any transaction will bring the database from one valid state to another
      - **Isolation**: executing transactions concurrently has the same results as if the transactions were executed serially.
      - **Durability**: guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure (e.g., power outage or crash).
    - **Reasons for SQL**: consistent no room for error, structured data, strict schema, relational data, need for complex joins, transactions, lookups by index are fast
  - **NoSQL**: data items represented in key-value store, document store, wide column store, or graph database. Data is denormalized, and joins are generally done in the application code. Most NoSQL stores lack true ACID transactions in favour of eventual consistency. Scales horizontally and higher availability.
    - **BASE**: set properties of NoSQL database. Basically Available, Soft state, Eventual consistency. BASE system chooses availabilty over consistency.
      - **Basically available**: gurantees availability
      - **Soft state**: state of the system may change over time, wven without input
      - **Eventual consistency**: the system will become consistent over time, given that the system doesn't receive input during that time.
    - **Reasons for NoSQL**: budget won't allow large devices and must be put into lower performance devices, datastructures being managed is variable, analyzing large quantities of data in read mode only, non-relational data, no need for complex joins, store many TB (or PB) of data
      - leaderboard or scoring data, temporary data like shopping cart, lookup tables, frequently accessed tables, log data
    - **Key-Value Store**: O(1) reads and writesand is often backed by memory or SSD.
      - Abstraction: hash table
      - High performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer. Since they offer a limited set of operations, complexity is shifted to the application layer if additional operations are needed.
      - Key-value store is the basis for more compex system such as document store and a graph database.
      - Key is auto-generated while value can be String, JSON, Blob etc.
      - Example: Amazon S3
    - **Document Store**: document store in centered around documents where a document stores all information for a given object. Docuent stores provide APIs or a query language to query based on the internal structure of the document itself.
      - Abstraction: key-value store with documents stored as values
      - Documents are organized by collections, tags, metadata, or directories. Although documents can be organized or grouped together, documents may have fields that are completely different from each other
      - documents stores provide a high flexibility and are often used for working with ocassionally changing data
      - document data is stored as XML, JSON, binary etc
      - schema-less, makes adding fields to JSON documents a simple task without having to define changes first
      - Example: MongoDB, CouchDB, DynamoDB
    - **Wide Column Store**: basic unit of data is a column (name/value pair). A column can be grouped in column families(analogous to a SQL table). Super column families further group column families. Column families can contain virtually unlimited number of columns that can be created at runtime or the definition of the schema. Each column can be accessed independently with a row key, and columns with the same row key form a row. Each value contains a timestamp for versioning and for conflict resolution.
      - Abstraction: nested map ColumnFamily<RowKey, Columns<ColKey, Value, Timestamp>>
      - Offer high availability and high scalability. They are often used for very large data sets.
      - Example: BigTable, HBase, Cassandra
    - **Graph Database**: each node is a record and each arc is a relationship between two nodes. Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.
      - Abstraction: graph
      - Offer high performance for data models with complex relationships, such as a social network.
      - Relatively new and are not yet widely-used; it might be more difficult to find development tools and resources.
      - Many graphs can only be accessed with REST APIs
      - Example: Neo4j, FlockDB
  - **Techniques to scale database**
    - **Replication**: frequent, automatic copying of data from a database of one computer or server to a database in another
      - **Master-slave replication**: master serves read and writes, replicating writes to one or more slaves, which serve only reads.Slaves can also replicate to additional slaves in a tree-like fashion. If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provided.
        - Disadvantages: additional logic is needed to promote a slave to master
      - **Master-master replication**: both masters serve reads and writes and coordinate with each other on writes. If either master goes down, the system can continue to operate with both reads and writes.
        - Disadvantages: need a loadbalancer or make changes to application logic to determine where to write, most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization, conflict resolution comes more into play as more write nodes are added and as latency increases
      - **Buddy Replication**: suppress replicating your data to all instances in a cluster. Instead, each instance picks one or more 'buddies' in the cluster, and only replicates to these specific buddies. This greatly helps scalability as there is no longer a memory and network traffic impact every time another instance is added to a cluster.
        - Benefitial only if a certain data is frequently accessed it is served from one instance rather than a round-robin fashion. Ex: sticky sessions
      - Disadvantages of replication:
        - potential for loss of data if the msater fails before any newly written data can be replicated to other nodes
        - since writes are replayed to read replicas they might get bogged down if there are a lot of writes and can't do as many reads
        - increasing read slaves increases time to replicate which increases replication lag,
        - replication adds more hardware and additional complexity
    - **Functional Partitioning**: (or federation) splits up database into several smaller databases by function. Results in less read and write traffic to each smaller database and therefore less replication lag. Smaller databases result in more data that can fit in memory, which result in more cache hits due to improved cache locality. With no single central master serializing writes can be done in parallel, increasing throughput. This is done for manageability, performance or availability reasons, or load balancing.
      - Disadvantages: not effective if schema requires huge functions or tables, update application logic to determine which database to read and write, joining two databases is more complex with a server link, partitioning adds more hardware and additional complexity
    - **Sharding**: distributes data across different databases such that each database can only manage a subset of the data. As data increases more shards are added to the cluster. Common ways to shard a table of users is either through the user's last name initial or the user's geographic location.
      - **Consistent hashing**: once range of keys are spread across the available nodes, find the right node with the hash code for a key. Performs sharding nicely and elegantly. Reduces the amount of transferred data.
      - Advantages: less read and write traffic, less replication, and more cache hits. Index size is reduced, which generally improves performance with faster queries. If one shard goes down, the other shards are still operational, although some form of replication needs to be in place to prevent data loss. There is no single central master serializing writes, allowing you to write in parallel with increased throughput.
      - Disadvantages: application logic needs to be updated to word with shards, this means complex SQL queries. Some shards may deal with more load compared to others, therefore rebalancing needs to be done which adds complexity. Joing data from multiple shards is more complex. Sharding adds more hardware and additional complexity.
    - **Denormalization**: improves read performance at the expense of write performance. Redundant copies of the data are written in multiple tables to avoid expensive joins.
      - Advantages: Once data is distributed through partition or sharding, managing data becomes complex. Denormalization might circumvent complex joins. Some read operations may be expensive due to join and this helps circumvent that.
      - Disadvantages: Generally there are more reads than write. Data is duplicated. Constraints can help redundant copies of information stay in sync, which increases comlexity. Denormalized database under heavy write load might perform worse than its normalized counterpart.
    - **SQL tuning**: need to benchmark and profile to simulate and uncover bottlenecks.
      - **Benchmark**: simulate high-load situations with tools such as *ab*. **Profile**: enable tools such as the *slow query log* to help track performance issues. Optimizations that can be done with benchmark and profiling are
        - **Tighten up the schema**
          - MySQL dumps to disk in contiguous blocks for fast access
          - Use *CHAR* instead of *VARCHAR* for fixed-length fields
          - Use *TEXT* for large blocks of text such as blog posts. *TEXT* also allows for boolean searches. Using a *TEXT* field results in storing a pointer on a disk that is used to locate the text block.
          - Use *INT* for larger numbers up to 2^32 or 4 billion
          - Use *DECIMAL* for currency to avoid floating point representation errors.
          - Avoid storing large *BLOBS*, store the location of where to get the object instead
          - *VARCHAR(255)* is the largest number of characters that can be counted in a 8 bit number, often maximizing the use of a byte in some RDBMS.
          - Set the *NOT NULL* constraint where applicable to improve search performance.
        - **Use good indices**
          - columns that are being queried could be faster with indices
          - indices are usually represented as self-balancing B-tree that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time
          - placing an index can keep the data in memory, requiring more space
          - writes could also be slower since the index also needs to be updated
          - when loading large amounts of data, it might be faster to disable indices, load the data, then rebuild the indices
        - **Avoid expensive joins**: denormalize where performance demands it
        - **Partition tables**: break up a table by putting hot spots in a seperate table to help keep it in memory.
        - **Tune the query cache**: in some cases the query cache could lead to performance issues.
    - [**More NoSQL patterns**](http://horicky.blogspot.com/2009/11/nosql-patterns.html)
- **Cache**: improves page load time and can reduce the load on servers and databases. Dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution. Putting a cache can absorb uneven loads and spikes in traffic. It speeds up request, if data is accessed frequently store it in cache so it can be retrieved quickly.
  - **Client caching**: can be located on the client side (OS or browser), server side, or in a distinct layer.
  - **CDN caching**: CDNs are considered a type of cache
  - **Web server caching**: reverse proxies and caches such as Varnish can serve static and dynamic content directly. Web servers can also cache requests, returning responses without having to contact application servers.
  - **Database caching**: database usually includes some level of caching in a default configuration, optimized for a generic use case. Tweaking these settings for specific usage patterns can further boost performance.
  - **Application caching**: In-memory caches such as Memcached and Redis are key-value stores between your application and your data storage. Since the data is held in RAM, it is much faster than typical databases where data is stored on disk. RAM is more limited than disk, so cache invalidation algorithms such as least recently used (LRU) can help invalidate 'cold' entries and keep 'hot' data in RAM Avoid file-based caching, as it makes cloning and auto-scaling more difficult. Cache can fall into two general categories: *database queries* and *objects*. These are row level, query-level, fully-formed serializable objects, fully-rendered HTML
    - **Caching at the database query level**: whenever the database is queried, hash the query as a key and store the result to the cache. This approach suffers from expiration issues:
      - Hard to delete a cached result with complex queries
      - If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell
    - **Caching at the object level**: see data as an object, similar to what is donw with application code. Application assembles the dataset from the database into a class instance or a data structure(s)
      - Remove the object from cache if its underlying data has changed
      - Allos for asynchronous processing: workers assemble objects by consuming the latest cached object
  - Caching algorithms
    - **Least Frequently Used(LFU)**: use counter to keep track of how often an entry is accessed. The entry with lowest count is removed first.
    - **Least Recently Used(LRU)**: recently used items kept near top of the cache.
    - **Most Recently Used(MRU)**: removes most recently used items first. Good when the older an item is the more likely it is to be accessed.
    - **Adaptive Replacement Cache(ARC)**: keeps track of both LFU and LRU, as well as evicted cache entries to get the best use out of the available cache.
  - **When to update cache**: only limited amount of data can be stored in cache, need to determine which update strategy is best.
    - **Cache-aside**: application is responsible for reading and writing from storage. The cache does not interact with storage directly. Cache-aside is also referred to as lazy loading. Only requested data is cached, which avoids filling up the cache that is not requested.
      - Process
        - look for entry in cache, resulting in a cache miss
        - load entry from the database
        - add entry to cache
        - return entry
      - Disadvantages
        - each cahce miss results in three trips, which can cause noticeable delay
        - data can become stale if it is updated in the database. This issue is mitigated by setting a time-to-live (TTL) which forces an update of cache entry, or by using write-through.
        - When a node fails, it is replaced by a new, empty node, increasing latency.
    - **Write-through**: application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database. Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast. Users are generally more tolerant of latency when updating data than reading data. Data in the cache is not stale.
      - Process
        - application adds/updates entry in cache
        - cache synchronously writes entry to data store
        - return entry
      - Disadvantages
        - When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database. Cache-aside in conjunction with write through can mitigate this issue.
        - Most data written might never be read, which can be minimized with a TTL.
    - **Write-behind(write-back)**:
      - Process
        - Add/update entry in cache
        - Asynchronously write entry to the data store, improving write performance
      - Disadvantages
        - There could be data loss if the cache goes down prior to its contents hitting the data store
        - It is more complex to implement write-behind than it is to implement cache-aside or write through
    - **Refresh-ahead**: can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future. Configure cache to automatically refresh any recently accessed cache entry prior to its expiration.
      - Process
        - Look for entry in cache
        - If entry not present get info from database and store entry in cache
        - Return entry
      - Disadvantages
        - Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh-ahead
  - What to cache: user sessions, fully rendered web pages, activity streams, user graph data(user<->friend relationships)
  - Disadvantages
    - Need to maintain consistency between caches and the source of truth such as the database through cache invalidation.
    - Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.
    - Need to make application changes such as adding Redis or memcached.
- **Load Balancer**: distribute incoming client requests to computing resources such as application servers and database. Can be implemented with hardware (expensive) or with software such as HAProxy. It is common to set up multiple load balancers, either in *active-passive* or *active-active* mode. Can help with horizontal scaling, improving performance and availability.
  - Effective at
    - Preventing requests from going to unhealthy servers
    - Preventing overloading resources
    - Helping eliminate single points of failure
    - **SSL termination**: decrypt incoming requests and encrypt server responses so backed servers do not have to perform these potentially expensive operations
    - **Session persistence**: issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions
  - Disadvantages
    - load balancer can become a bottleneck if it does not have enough resources or if it is not configured properly
    - introducing a load balancer to help eliminate single points of failure results in increased complexity
    - single load balancer is a single point of failure, configuring multiple load balancers further increases complexity
  - Can route traffic based on various metrics, including:
    - **Random**
    - **Least loaded**
    - **Session/cookies**
    - **Round robin or weighted round robin**: each server is assigned a value relative to other servers in the pool. This "weight" determines how many more or fewer requests are sent that server's way; compared to other servers in the pools
    - **Layer 4**: looks at *transport layer* from OSI layer model to decide how to distribute requests. Involves source and destination IP addresses, and ports in the header, but not contents of the packet.
    - **Layer 7**: looks at *application layer* from OSI layer model to decide how to distribute requests. This can involve contents of the header, message, and cookies. It can terminate network traffic, read message, make load-balancing decision, then open a connection to the selected server. Layer 4 requires less time and computing resources at the cost of flexibility, but the performance impact is minimal on hardware. Layer 7 is more preferred.
- **Reverse Proxy (Web Server)**: web server that centralizes internal services and provides unified interfaces to the public. Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.
  - Benefits
    - **Increased security**: hide information about backend servers, blacklist IPs, limit number of connections per client
    - **Increased scalability and flexibility**: clients only see the reverse proxy's IP, allowing to scale servers or change their configuration
    - **SSL termination**: decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations
    - **Compression**: compress server responses
    - **Caching**: return the response for cached requests
    - **Static content**: server static content directly. Ex: html/css/js, photos etc
  - Disadvantages
    - results in increased complexity
    - single proxy is a single point of failure, configuring multiple reverse proxies (ie failover) further increases complexity
- **Load Balancer vs Reverse Proxy**: load balancer is useful for multiple servers while reverse proxy can be useful with even just one web server or application server
- **Content Delivery Network (CDN)**: globally distributed network of proxy servers, serving from locations closer to the user. Generally, it is static content such as HTML, CSS, JS, photos, videos, but it can also be dynamic content.
  - Advantages
    - Users receive content from data centers close to them
    - Servers do not have to serve requests that the CDN fulfills
  - Disadvantages
    - CDN cost could be significant depending on traffic
    - Content might be stale if it is updated before the time-to-live(TTL) expires it.
    - CDNs require changing URLs for static content to point to the CDN
  - **Push CDNs**: receive content whenever changes occur on server. You take full responsibility for providing content, uploading directly to CDN, and rewriting URLs to point to CDNS.
    - Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.
    - Sites with small amount of traffic or sites with content that isn't often updated work well. Content is placed on CDNs once, instead of being re-pulled at regular intervals.
  - **Pull CDNs**: grab new content from server when the first user requests it. Leave content on the server and rewrite URLs to point to the CDN. Results in slower request until content is cached on the CDN. TTL determines how long content is cached.
    - Minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.
    - Sites with heavy traffic work well, as traffic is spread out more evenly with only recently-requested content remaining on CDN.
- **Communication**
  - **OSI 7 layer model**: Open System Interconnection, model defines a networking framework for implementing protocols in seven layers
    1. Physical: transmission of data using physical structures such as cables, hub etc.
    2. Data Link: error free transmission of data frames from one node to another
    3. Network: decides which physical path the data takes. Deals with packets
    4. Transport: ensures messages are delivered error-free, in sequence, and with no losses or duplication. Includes TCP
    5. Session: allows session establishment between processes running. Includes logical ports
    6. Presentation: formats data to be presented to application layer. Encrypts and decrypts data
    7. Application: serves the window for users and application processes to access network services.
  - **Hypertext Transfer Protocol (HTTP)**: is a method for encoding and transporting data between a client and a server. It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request.
    - HTTP is an application layer protocol relying on lower-level protocols such as TCP and UDP.
    - Common HTTP terms
      - GET: Reads a resource
      - POST: Creates a resource or trigger a process that handles data
      - PUT: Creates or replace a resource
      - PATCH: Partially updates a resource
      - DELETE: Deletes a resource
    - **Hypertext Transfer Protocol Secure (HTTPS)**: the secure version of HTTP. All communications between your browser and the website are encrypted.
  - **Transmission Control Protocol (TCP)**: connection-oriented protocol over an IP network. Connection is established using a handshake. All packets sent are guaranteed to reac the destination in the original order and without corruption. If the sender does not receive a correct response, it will resend the packets. If there are multiple timeouts, the connection is dropped. TCP also implements flow control and congestion control. These guarantees cause delays and generally result in less efficient transmission than UDP.
    - Used: applications that require high reliability but are less time critical. Some examples include web servers, database info, SMTP, FTP, and SSH.
    - TCP over UDP when: all the data needs to arrive intact, automatically make a best estimate use of the network throughput
  - **User Datagram Protocol (UDP)**
  - **Remote Procedure Call (RPC)**
  - **Representational State Transfer (REST)**
  - **GraphQl**
- **Asynchronism**: asynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line. They can also help by doing time-consuming work in advance, such as periodic aggregation of data.
  - **Message queues**: message queues receive, hold, and deliver messages. If an operation is too slow to perform inline, you can use a message queue.
    - The user is not blocked and the job is processed in the background. During this time, the client might optionally do a small amout of processing to make it seem like the task has completed. For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.
    - Process
      - An application publishes a job to the queue, then notifies the user of job status
      - A worker picks up the jobs from the queue, processes it, then signals the job is completed
    - Example: Redis, RabbitMQ, Amazon SQS
  - **Task queues**: receive tasks and their related data, runs them, then delivers their results. They can support scheduling and can be used to run computationaly-intensive jobs in the background.
    - Example: celery
  - **Back pressure**: limits queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue. Once the queue fills up, clients get a server busy of HTTP 503 status code to try again later.
  - Disadvantages
    - use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations, as introducing queues can add delays and complexity.
- **Domain Name System (DNS)**: translates a domain name such as www.example.com to IP address. 
  - Disadvantages: slight delay, DNS server management could be complex (done by government, ISPs), Denial of Service(DDoS) attack
  - Services like CloudFlare can route traffics through various methods
    - [**Weighted round robin**](http://g33kinfo.com/info/archives/2657): each server is assigned a value relative to other servers in the pool. This "weight" determines how many more or fewer requests are sent that server's way; compared to other servers in the pools
      - Prevent traffic from going to servers under maintenance
      - Balance between varying cluster sizes
      - A/B testing
    - Latency based
    - Geolocation-based
  - **NX record (name server)**: specifies the DNS servers for your domain/subdomain
  - **MX record (mail exchange)**: specifies the mail servers for accepting messages.
  - **A record (address)**: points a name to an IP address
  - **CNAME (canonical)**: Points a name to another name, CNAME (example.com to www.example.com) or to an A record
- **Readundant Array of Independent Disks(RAID)**: assumes there are multiple hard drives.
  - **RAID 0**: segment logically sequential data between 2 hard drives (striping). This decreases write time
  - **RAID 1**: mirror data, write data in two places in parallel. This way even if one drive dies the other can still serve data.
  - **RAID 5**:  4-5 drives and one is used for redundancy. If one fails can be retrieved
  - **RAID 6**: 4-5 drives and one is used for redundancy. If 1-2 fails can be retrieved
  - **RAID 10**: four drives. Combination of Raid 0 and Raid 1, where it provides both striping and redundancy.
- **Security**: is a broad topic but general ideas
  - Encrypt in transit and at rest.
  - Sanitize all user inputs or any input parameters exposed to user to prevent XSS and SQL injection.
  - Use parameterized queries to prevent SQL injection.
  - **Principle of least privilege**: every module must be able to access only the information and resources that are necessary for its legitimate purpose.

## Sources

- [System Design Primer](https://github.com/donnemartin/system-design-primer)
- [Scalability, Availability & Stability Patterns](https://www.slideshare.net/jboner/scalability-availability-stability-patterns)
- [System Design Interview](https://github.com/checkcheckzz/system-design-interview)
- [System Design](https://github.com/shashank88/system_design)

## Additional sources to consult

- https://lethain.com/introduction-to-architecting-systems-for-scale/
- https://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview
- http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html
- http://horicky.blogspot.com/2008/02/scalable-system-design.html
- Microservices
- Zookeeper
- NGINX
- Web 1.0, 2.0, 3.0
- Operating system
- add blockchain to database
- previous scalability notes
- general designs different companies
- desgin patterns
- tools to know
- microservices
- websocket vs polling
- graphql
- redis
- memcached
- rabbitMQ
- Amazon SQS
- https://www.infoq.com/articles/ebay-scalability-best-practices
- http://srinathsview.blogspot.com/2011/10/list-of-known-scalable-architecture.html
- https://www.ibm.com/developerworks/rational/tutorials/ar-designpat2/index.html
